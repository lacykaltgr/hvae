{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# hVAE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhvae\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hVAE\n\u001B[1;32m      3\u001B[0m PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 5\u001B[0m hvae \u001B[38;5;241m=\u001B[39m \u001B[43mhVAE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m hvae\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m      7\u001B[0m hvae\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m10\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: load() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "from src.hvae import hVAE\n",
    "\n",
    "PATH = \"path\"\n",
    "\n",
    "hvae = hVAE.load(PATH)\n",
    "hvae.eval()\n",
    "hvae.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "\n",
    "vae = VAE()\n",
    "for p in vae.parameters():\n",
    "    print(p.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_graph():\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    encoder_edges = [('x', 'z1'), ('y', 'z1'), ('x', 'y')]\n",
    "    decoder_edges = [('z1', 'x'), ('y', 'z1')]\n",
    "\n",
    "    # use the same node list for both nx.draw calls\n",
    "    nodes = ['x', 'y', 'z1']\n",
    "\n",
    "    # specify the positions of the nodes\n",
    "    pos = {'x': (0, 0), 'z1': (0, 1), 'y': (0, 2)}\n",
    "\n",
    "    nx.draw(G, nodelist=nodes, pos=pos, edgelist=encoder_edges, edge_color=\"r\", width=2, node_size=2000, with_labels=True, node_color=\"lightblue\", ax=plt.gca(), arrowstyle=\"->\", connectionstyle=\"arc3,rad=0.2\")\n",
    "\n",
    "    # draw the blue edges with labels and arrows on the same axis with negative curvature\n",
    "    nx.draw(G, nodelist=nodes, pos=pos, edgelist=decoder_edges, edge_color=\"b\", width=2, node_size=2000, with_labels=True, node_color=\"lightblue\", ax=plt.gca(), arrowstyle=\"->\")\n",
    "    # show the plot\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_graph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from hparams import get_hparams\n",
    "\n",
    "hparams = get_hparams()\n",
    "model = hparams.model_params.model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Checkpoint is loaded\n",
      "Validation Stats| Reconstruction Loss 37.4020 | KL Div 95188.7109 |NELBO 95226.109375 | SSIM: 0.144781\n"
     ]
    }
   ],
   "source": [
    "from test import main\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 20:17:35,056 - INFO - Train step generator trainable params 18.361m.\n",
      "2023-08-11 20:17:35,115 - INFO - (0, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.748), ('KL loss', 100504.992), ('ELBO', 25.7141), ('average KL loss', 205834.243), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 30283430.0), ('GradSkipCount', 1.0))\n",
      "2023-08-11 20:17:35,116 - INFO - \n",
      "-----------------------------------------------------------------------------------------\n",
      "2023-08-11 20:17:35,126 - INFO - Train Stats for global_step 0 | NELBO 25.714069366455078 | SSIM: 0.1487065851688385\n",
      "2023-08-11 20:17:38,888 - INFO - Validation Stats| Reconstruction Loss 37.4025 | KL Div 257589.0000 |NELBO 257626.406250 | SSIM: 0.145304\n",
      "2023-08-11 20:17:38,889 - INFO - Logging to Tensorboard..\n",
      "2023-08-11 20:17:43,735 - INFO - Saved checkpoint for global_step 0 to experiments/TDVAE/2023-08-11__20-17/checkpoints/checkpoint-0.pth\n",
      "2023-08-11 20:17:43,736 - INFO - \n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "2023-08-11 20:17:43,793 - INFO - (1, ('Time/Step (sec)', 0.06), ('Reconstruction Loss', 18.736), ('KL loss', 549182.125), ('ELBO', 56.802), ('average KL loss', 1124725.11), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 255898848.0), ('GradSkipCount', 2.0))\n",
      "2023-08-11 20:17:43,846 - INFO - (2, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.648), ('KL loss', 1510.56), ('ELBO', 18.7526), ('average KL loss', 3093.626), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 67465.3), ('GradSkipCount', 3.0))\n",
      "2023-08-11 20:17:43,899 - INFO - (3, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.743), ('KL loss', 7926.413), ('ELBO', 19.2925), ('average KL loss', 16233.292), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 453519.8), ('GradSkipCount', 4.0))\n",
      "2023-08-11 20:17:43,951 - INFO - (4, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.744), ('KL loss', 1160.538), ('ELBO', 18.8242), ('average KL loss', 2376.782), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 5534.6), ('GradSkipCount', 5.0))\n",
      "2023-08-11 20:17:44,003 - INFO - (5, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.724), ('KL loss', 6939.746), ('ELBO', 19.2051), ('average KL loss', 14212.601), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 328770.2), ('GradSkipCount', 6.0))\n",
      "2023-08-11 20:17:44,004 - INFO - \n",
      "-----------------------------------------------------------------------------------------\n",
      "2023-08-11 20:17:44,014 - INFO - Train Stats for global_step 5 | NELBO 19.205053329467773 | SSIM: 0.1432485729455948\n",
      "2023-08-11 20:17:47,276 - INFO - Validation Stats| Reconstruction Loss 37.4023 | KL Div 14770926.0000 |NELBO 14770963.000000 | SSIM: 0.145405\n",
      "2023-08-11 20:17:47,277 - INFO - Logging to Tensorboard..\n",
      "2023-08-11 20:17:51,791 - INFO - Saved checkpoint for global_step 5 to experiments/TDVAE/2023-08-11__20-17/checkpoints/checkpoint-5.pth\n",
      "2023-08-11 20:17:51,792 - INFO - \n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "2023-08-11 20:17:51,844 - INFO - (6, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.723), ('KL loss', 702.482), ('ELBO', 18.7715), ('average KL loss', 1438.683), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 3447.0), ('GradSkipCount', 7.0))\n",
      "2023-08-11 20:17:51,895 - INFO - (7, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.766), ('KL loss', 60100.055), ('ELBO', 22.9318), ('average KL loss', 123084.911), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 6182323.5), ('GradSkipCount', 8.0))\n",
      "2023-08-11 20:17:51,946 - INFO - (8, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.742), ('KL loss', 1249.073), ('ELBO', 18.8283), ('average KL loss', 2558.102), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 17276.6), ('GradSkipCount', 9.0))\n",
      "2023-08-11 20:17:51,999 - INFO - (9, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.761), ('KL loss', 9334.339), ('ELBO', 19.4085), ('average KL loss', 19116.73), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 577086.5), ('GradSkipCount', 10.0))\n",
      "2023-08-11 20:17:52,050 - INFO - (10, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.724), ('KL loss', 1384.107), ('ELBO', 18.8196), ('average KL loss', 2834.65), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 7341.8), ('GradSkipCount', 11.0))\n",
      "2023-08-11 20:17:52,050 - INFO - \n",
      "-----------------------------------------------------------------------------------------\n",
      "2023-08-11 20:17:52,060 - INFO - Train Stats for global_step 10 | NELBO 18.819622039794922 | SSIM: 0.14578606188297272\n",
      "2023-08-11 20:17:55,307 - INFO - Validation Stats| Reconstruction Loss 37.4021 | KL Div 219962.2031 |NELBO 219999.609375 | SSIM: 0.145163\n",
      "2023-08-11 20:17:55,308 - INFO - Logging to Tensorboard..\n",
      "2023-08-11 20:17:59,881 - INFO - Saved checkpoint for global_step 10 to experiments/TDVAE/2023-08-11__20-17/checkpoints/checkpoint-10.pth\n",
      "2023-08-11 20:17:59,881 - INFO - \n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "2023-08-11 20:17:59,935 - INFO - (11, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.603), ('KL loss', 2172.672), ('ELBO', 18.7536), ('average KL loss', 4449.632), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 126058.0), ('GradSkipCount', 12.0))\n",
      "2023-08-11 20:17:59,987 - INFO - (12, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.721), ('KL loss', 11131.828), ('ELBO', 19.4929), ('average KL loss', 22797.985), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 668119.9), ('GradSkipCount', 13.0))\n",
      "2023-08-11 20:18:00,041 - INFO - (13, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.719), ('KL loss', 1052.971), ('ELBO', 18.7917), ('average KL loss', 2156.485), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 12605.4), ('GradSkipCount', 14.0))\n",
      "2023-08-11 20:18:00,092 - INFO - (14, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.717), ('KL loss', 1245.813), ('ELBO', 18.8031), ('average KL loss', 2551.424), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 11898.9), ('GradSkipCount', 15.0))\n",
      "2023-08-11 20:18:00,146 - INFO - (15, ('Time/Step (sec)', 0.05), ('Reconstruction Loss', 18.765), ('KL loss', 2220.443), ('ELBO', 18.9194), ('average KL loss', 4547.467), ('Beta', 0.0001), ('N° active groups', 2), ('GradNorm', 19668.4), ('GradSkipCount', 16.0))\n",
      "2023-08-11 20:18:00,146 - INFO - \n",
      "-----------------------------------------------------------------------------------------\n",
      "2023-08-11 20:18:00,156 - INFO - Train Stats for global_step 15 | NELBO 18.919389724731445 | SSIM: 0.14137497544288635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m main\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/ai/hvae/train.py:54\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     51\u001B[0m writer_train \u001B[38;5;241m=\u001B[39m create_tb_writer_for(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, checkpoint_path\u001B[38;5;241m=\u001B[39mcheckpoint_path)\n\u001B[1;32m     52\u001B[0m writer_val \u001B[38;5;241m=\u001B[39m create_tb_writer_for(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m, checkpoint_path\u001B[38;5;241m=\u001B[39mcheckpoint_path)\n\u001B[0;32m---> 54\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschedule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgloabal_step\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m      \u001B[49m\u001B[43mwriter_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/ai/hvae/src/model.py:272\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(net, optimizer, schedule, train_loader, val_loader, checkpoint_start_step, tb_writer_train, tb_writer_val, checkpoint_path, logger)\u001B[0m\n\u001B[1;32m    269\u001B[0m train_ssim \u001B[38;5;241m=\u001B[39m ssim_metric(train_inputs, train_outputs, global_batch_size\u001B[38;5;241m=\u001B[39mprms\u001B[38;5;241m.\u001B[39mtrain_params\u001B[38;5;241m.\u001B[39mbatch_size)\n\u001B[1;32m    270\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain Stats for global_step \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mglobal_step\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | NELBO \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124melbo\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSSIM: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_ssim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 272\u001B[0m val_results, val_outputs, val_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglobal_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;66;03m# Tensorboard logging\u001B[39;00m\n\u001B[1;32m    274\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLogging to Tensorboard..\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/code/ai/hvae/src/model.py:332\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(net, val_loader, global_step, logger)\u001B[0m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m val_step, val_inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(val_loader):\n\u001B[1;32m    330\u001B[0m     val_inputs \u001B[38;5;241m=\u001B[39m val_inputs\u001B[38;5;241m.\u001B[39mto(device, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    331\u001B[0m     val_outputs, val_computed, val_results \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m--> 332\u001B[0m         \u001B[43mreconstruction_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mglobal_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    334\u001B[0m     val_ssim_per_batch \u001B[38;5;241m=\u001B[39m ssim_metric(val_inputs, val_outputs, global_batch_size\u001B[38;5;241m=\u001B[39mprms\u001B[38;5;241m.\u001B[39meval_params\u001B[38;5;241m.\u001B[39mbatch_size)\n\u001B[1;32m    335\u001B[0m     val_feature_matching_losses \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m val_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreconstruction_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/code/ai/hvae/src/model.py:109\u001B[0m, in \u001B[0;36mreconstruction_step\u001B[0;34m(net, inputs, variates_masks, step_n)\u001B[0m\n\u001B[1;32m    107\u001B[0m net\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 109\u001B[0m     predictions, computed, distributions \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariates_masks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_n \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    111\u001B[0m         step_n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(prms\u001B[38;5;241m.\u001B[39mloss_params\u001B[38;5;241m.\u001B[39mvae_beta_anneal_steps, prms\u001B[38;5;241m.\u001B[39mloss_params\u001B[38;5;241m.\u001B[39mgamma_max_steps) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m10.\u001B[39m\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/ai/hvae/src/hvae.py:131\u001B[0m, in \u001B[0;36mhVAE.forward\u001B[0;34m(self, x, variate_masks)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: tensor, variate_masks\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m (tensor, \u001B[38;5;28mdict\u001B[39m, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    130\u001B[0m     _, computed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_block(x)\n\u001B[0;32m--> 131\u001B[0m     _, computed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcomputed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    132\u001B[0m     _, computed, distributions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(computed, variate_masks)\n\u001B[1;32m    133\u001B[0m     output, computed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_block(computed)\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/ai/hvae/src/hvae.py:19\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, x, to_compute)\u001B[0m\n\u001B[1;32m     17\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder_blocks:\n\u001B[0;32m---> 19\u001B[0m     output, computed \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcomputed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m to_compute \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m to_compute \u001B[38;5;129;01min\u001B[39;00m computed:\n\u001B[1;32m     21\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m computed[to_compute], computed\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/ai/hvae/src/block.py:81\u001B[0m, in \u001B[0;36mEncBlock.forward\u001B[0;34m(self, computed)\u001B[0m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in computed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     80\u001B[0m inputs \u001B[38;5;241m=\u001B[39m computed[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput]\n\u001B[0;32m---> 81\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m computed[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput] \u001B[38;5;241m=\u001B[39m output\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output, computed\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/ai/hvae/src/elements/nets.py:63\u001B[0m, in \u001B[0;36mMLPNet.forward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs):\n\u001B[1;32m     62\u001B[0m     x \u001B[38;5;241m=\u001B[39m inputs\n\u001B[0;32m---> 63\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp_layers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresidual:\n\u001B[1;32m     65\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m inputs \u001B[38;5;241m+\u001B[39m x\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda/envs/tensyflow/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from train import main\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 20:39:30,604 - INFO - Model Checkpoint is loaded\n",
      "2023-08-11 20:39:30,685 - INFO - Step: 0000  | NELBO: 18185.6055 | Reconstruction: 37.5274 | kl_div: 26182.1426| SSIM: 0.2906 \n",
      "2023-08-11 20:39:30,731 - INFO - Step: 0001  | NELBO: 1390.3896 | Reconstruction: 37.4015 | kl_div: 1951.9493| SSIM: 0.2866 \n",
      "2023-08-11 20:39:30,777 - INFO - Step: 0002  | NELBO: 15590.1328 | Reconstruction: 37.3964 | kl_div: 22437.8555| SSIM: 0.2894 \n",
      "2023-08-11 20:39:30,822 - INFO - Step: 0003  | NELBO: 4945.2681 | Reconstruction: 37.5371 | kl_div: 7080.3589| SSIM: 0.2897 \n",
      "2023-08-11 20:39:30,866 - INFO - Step: 0004  | NELBO: 74634.1562 | Reconstruction: 37.3911 | kl_div: 107620.3828| SSIM: 0.2938 \n",
      "2023-08-11 20:39:30,910 - INFO - Step: 0005  | NELBO: 7136.1167 | Reconstruction: 37.3642 | kl_div: 10241.3350| SSIM: 0.2967 \n",
      "2023-08-11 20:39:30,954 - INFO - Step: 0006  | NELBO: 74096.5156 | Reconstruction: 37.4514 | kl_div: 106844.6406| SSIM: 0.2846 \n",
      "2023-08-11 20:39:30,999 - INFO - Step: 0007  | NELBO: 12732.3438 | Reconstruction: 37.5175 | kl_div: 18314.7637| SSIM: 0.2882 \n",
      "2023-08-11 20:39:31,044 - INFO - Step: 0008  | NELBO: 840129.0000 | Reconstruction: 37.1301 | kl_div: 1211996.3750| SSIM: 0.2904 \n",
      "2023-08-11 20:39:31,087 - INFO - Step: 0009  | NELBO: 892.5518 | Reconstruction: 37.4794 | kl_div: 1233.6086| SSIM: 0.2957 \n",
      "2023-08-11 20:39:31,131 - INFO - Step: 0010  | NELBO: 2857.8354 | Reconstruction: 37.3723 | kl_div: 4069.0681| SSIM: 0.2932 \n",
      "2023-08-11 20:39:31,182 - INFO - Step: 0011  | NELBO: 45506.6172 | Reconstruction: 37.3709 | kl_div: 65598.2578| SSIM: 0.2879 \n",
      "2023-08-11 20:39:31,225 - INFO - Step: 0012  | NELBO: 1866.1217 | Reconstruction: 37.4493 | kl_div: 2638.2166| SSIM: 0.2919 \n",
      "2023-08-11 20:39:31,268 - INFO - Step: 0013  | NELBO: 1734.0221 | Reconstruction: 37.2750 | kl_div: 2447.8887| SSIM: 0.2914 \n",
      "2023-08-11 20:39:31,313 - INFO - Step: 0014  | NELBO: 1129140.7500 | Reconstruction: 37.3233 | kl_div: 1628951.8750| SSIM: 0.2924 \n",
      "2023-08-11 20:39:31,356 - INFO - Step: 0015  | NELBO: 973.6723 | Reconstruction: 37.5312 | kl_div: 1350.5662| SSIM: 0.2894 \n",
      "2023-08-11 20:39:31,400 - INFO - Step: 0016  | NELBO: 8540.3672 | Reconstruction: 37.4376 | kl_div: 12267.1348| SSIM: 0.2905 \n",
      "2023-08-11 20:39:31,444 - INFO - Step: 0017  | NELBO: 597824.4375 | Reconstruction: 37.4918 | kl_div: 862424.2500| SSIM: 0.2928 \n",
      "2023-08-11 20:39:31,488 - INFO - Step: 0018  | NELBO: 2772.2222 | Reconstruction: 37.4055 | kl_div: 3945.5063| SSIM: 0.2909 \n",
      "2023-08-11 20:39:31,532 - INFO - Step: 0019  | NELBO: 18919.3828 | Reconstruction: 37.4668 | kl_div: 27240.8457| SSIM: 0.2885 \n",
      "2023-08-11 20:39:31,577 - INFO - Step: 0020  | NELBO: 9195.5371 | Reconstruction: 37.3876 | kl_div: 13212.4170| SSIM: 0.2844 \n",
      "2023-08-11 20:39:31,623 - INFO - Step: 0021  | NELBO: 170027.9375 | Reconstruction: 37.3456 | kl_div: 245244.5781| SSIM: 0.2942 \n",
      "2023-08-11 20:39:31,667 - INFO - Step: 0022  | NELBO: 70919.8672 | Reconstruction: 37.3734 | kl_div: 102261.8203| SSIM: 0.2852 \n",
      "2023-08-11 20:39:31,711 - INFO - Step: 0023  | NELBO: 2908.2915 | Reconstruction: 37.4034 | kl_div: 4141.8159| SSIM: 0.2889 \n",
      "2023-08-11 20:39:31,755 - INFO - Step: 0024  | NELBO: 7810.7637 | Reconstruction: 37.4314 | kl_div: 11214.5479| SSIM: 0.2858 \n",
      "2023-08-11 20:39:31,810 - INFO - Step: 0025  | NELBO: 4987.3931 | Reconstruction: 37.4946 | kl_div: 7141.1938| SSIM: 0.2858 \n",
      "2023-08-11 20:39:31,857 - INFO - Step: 0026  | NELBO: 1064.5988 | Reconstruction: 37.4672 | kl_div: 1481.8376| SSIM: 0.2915 \n",
      "2023-08-11 20:39:31,901 - INFO - Step: 0027  | NELBO: 63954408.0000 | Reconstruction: 37.3217 | kl_div: 92266656.0000| SSIM: 0.2913 \n",
      "2023-08-11 20:39:31,945 - INFO - Step: 0028  | NELBO: 1550.0337 | Reconstruction: 37.4755 | kl_div: 2182.1602| SSIM: 0.2959 \n",
      "2023-08-11 20:39:31,990 - INFO - Step: 0029  | NELBO: 12523.3750 | Reconstruction: 37.3730 | kl_div: 18013.4922| SSIM: 0.2930 \n",
      "2023-08-11 20:39:32,034 - INFO - Step: 0030  | NELBO: 16044.2686 | Reconstruction: 37.3634 | kl_div: 23093.0820| SSIM: 0.2929 \n",
      "2023-08-11 20:39:32,077 - INFO - Step: 0031  | NELBO: 3704.1589 | Reconstruction: 37.5024 | kl_div: 5289.8672| SSIM: 0.2925 \n",
      "2023-08-11 20:39:32,121 - INFO - Step: 0032  | NELBO: 6700.6382 | Reconstruction: 37.3944 | kl_div: 9613.0283| SSIM: 0.2881 \n",
      "2023-08-11 20:39:32,164 - INFO - Step: 0033  | NELBO: 20709.3262 | Reconstruction: 37.5036 | kl_div: 29823.1348| SSIM: 0.2905 \n",
      "2023-08-11 20:39:32,207 - INFO - Step: 0034  | NELBO: 11989.4551 | Reconstruction: 37.3614 | kl_div: 17243.2266| SSIM: 0.2952 \n",
      "2023-08-11 20:39:32,250 - INFO - Step: 0035  | NELBO: 2517.2834 | Reconstruction: 37.2136 | kl_div: 3577.9844| SSIM: 0.2916 \n",
      "2023-08-11 20:39:32,295 - INFO - Step: 0036  | NELBO: 31006.9180 | Reconstruction: 37.5394 | kl_div: 44679.3711| SSIM: 0.2896 \n",
      "2023-08-11 20:39:32,339 - INFO - Step: 0037  | NELBO: 2776.2732 | Reconstruction: 37.4478 | kl_div: 3951.2898| SSIM: 0.2900 \n",
      "2023-08-11 20:39:32,384 - INFO - Step: 0038  | NELBO: 3916.2773 | Reconstruction: 37.4742 | kl_div: 5595.9302| SSIM: 0.2937 \n",
      "2023-08-11 20:39:32,426 - INFO - Step: 0039  | NELBO: 12637.8447 | Reconstruction: 37.3095 | kl_div: 18178.7305| SSIM: 0.2903 \n",
      "2023-08-11 20:39:32,469 - INFO - Step: 0040  | NELBO: 36948.9102 | Reconstruction: 37.1968 | kl_div: 53252.3477| SSIM: 0.2897 \n",
      "2023-08-11 20:39:32,515 - INFO - Step: 0041  | NELBO: 53597.9805 | Reconstruction: 37.4859 | kl_div: 77271.4609| SSIM: 0.2931 \n",
      "2023-08-11 20:39:32,558 - INFO - Step: 0042  | NELBO: 28731.9688 | Reconstruction: 37.4778 | kl_div: 41397.3984| SSIM: 0.2930 \n",
      "2023-08-11 20:39:32,602 - INFO - Step: 0043  | NELBO: 8444.4209 | Reconstruction: 37.2339 | kl_div: 12129.0078| SSIM: 0.2856 \n",
      "2023-08-11 20:39:32,645 - INFO - Step: 0044  | NELBO: 18954.3945 | Reconstruction: 37.4606 | kl_div: 27291.3652| SSIM: 0.2934 \n",
      "2023-08-11 20:39:32,688 - INFO - Step: 0045  | NELBO: 70512.3984 | Reconstruction: 37.4336 | kl_div: 101673.8906| SSIM: 0.2935 \n",
      "2023-08-11 20:39:32,731 - INFO - Step: 0046  | NELBO: 442214.0000 | Reconstruction: 37.4027 | kl_div: 637926.0000| SSIM: 0.2936 \n",
      "2023-08-11 20:39:32,775 - INFO - Step: 0047  | NELBO: 2172.8838 | Reconstruction: 37.4418 | kl_div: 3080.7917| SSIM: 0.2876 \n",
      "2023-08-11 20:39:32,819 - INFO - Step: 0048  | NELBO: 1515.7200 | Reconstruction: 37.4767 | kl_div: 2132.6543| SSIM: 0.2939 \n",
      "2023-08-11 20:39:32,864 - INFO - Step: 0049  | NELBO: 890028.8125 | Reconstruction: 37.4233 | kl_div: 1283986.1250| SSIM: 0.2864 \n",
      "2023-08-11 20:39:32,907 - INFO - Step: 0050  | NELBO: 79470.8047 | Reconstruction: 37.4077 | kl_div: 114598.1719| SSIM: 0.2863 \n",
      "2023-08-11 20:39:32,950 - INFO - Step: 0051  | NELBO: 70171.9609 | Reconstruction: 37.3509 | kl_div: 101182.8516| SSIM: 0.2897 \n",
      "2023-08-11 20:39:32,995 - INFO - Step: 0052  | NELBO: 79767.5156 | Reconstruction: 37.3671 | kl_div: 115026.2891| SSIM: 0.2873 \n",
      "2023-08-11 20:39:33,039 - INFO - Step: 0053  | NELBO: 8176.6055 | Reconstruction: 37.4568 | kl_div: 11742.3086| SSIM: 0.2888 \n",
      "2023-08-11 20:39:33,083 - INFO - Step: 0054  | NELBO: 1265.3988 | Reconstruction: 37.3627 | kl_div: 1771.6816| SSIM: 0.2976 \n",
      "2023-08-11 20:39:33,127 - INFO - Step: 0055  | NELBO: 156081248.0000 | Reconstruction: 37.3689 | kl_div: 225177600.0000| SSIM: 0.2937 \n",
      "2023-08-11 20:39:33,173 - INFO - Step: 0056  | NELBO: 9894.8525 | Reconstruction: 37.4900 | kl_div: 14221.1680| SSIM: 0.2882 \n",
      "2023-08-11 20:39:33,217 - INFO - Step: 0057  | NELBO: 13482.7803 | Reconstruction: 37.4036 | kl_div: 19397.5781| SSIM: 0.2944 \n",
      "2023-08-11 20:39:33,261 - INFO - Step: 0058  | NELBO: 15720.8252 | Reconstruction: 37.2588 | kl_div: 22626.6035| SSIM: 0.2852 \n",
      "2023-08-11 20:39:33,305 - INFO - Step: 0059  | NELBO: 449071.2812 | Reconstruction: 37.5022 | kl_div: 647818.8125| SSIM: 0.2899 \n",
      "2023-08-11 20:39:33,350 - INFO - Step: 0060  | NELBO: 374827.5000 | Reconstruction: 37.4336 | kl_div: 540707.7500| SSIM: 0.2937 \n",
      "2023-08-11 20:39:33,395 - INFO - Step: 0061  | NELBO: 99866.9062 | Reconstruction: 37.4019 | kl_div: 144023.5312| SSIM: 0.2882 \n",
      "2023-08-11 20:39:33,438 - INFO - Step: 0062  | NELBO: 17792.4277 | Reconstruction: 37.4529 | kl_div: 25615.0137| SSIM: 0.2872 \n",
      "2023-08-11 20:39:33,483 - INFO - Step: 0063  | NELBO: 5822.9038 | Reconstruction: 37.5342 | kl_div: 8346.5244| SSIM: 0.2899 \n",
      "2023-08-11 20:39:33,528 - INFO - Step: 0064  | NELBO: 976184.6875 | Reconstruction: 37.4854 | kl_div: 1408282.7500| SSIM: 0.2861 \n",
      "2023-08-11 20:39:33,572 - INFO - Step: 0065  | NELBO: 7851.2104 | Reconstruction: 37.3700 | kl_div: 11272.9883| SSIM: 0.2869 \n",
      "2023-08-11 20:39:33,618 - INFO - Step: 0066  | NELBO: 31456.0703 | Reconstruction: 37.4555 | kl_div: 45327.4805| SSIM: 0.2876 \n",
      "2023-08-11 20:39:33,662 - INFO - Step: 0067  | NELBO: 38828.7852 | Reconstruction: 37.5235 | kl_div: 55963.9609| SSIM: 0.2893 \n",
      "2023-08-11 20:39:33,705 - INFO - Step: 0068  | NELBO: 2321.3518 | Reconstruction: 37.2731 | kl_div: 3295.2288| SSIM: 0.2892 \n",
      "2023-08-11 20:39:33,750 - INFO - Step: 0069  | NELBO: 87533.1719 | Reconstruction: 37.3914 | kl_div: 126229.7266| SSIM: 0.2920 \n",
      "2023-08-11 20:39:33,795 - INFO - Step: 0070  | NELBO: 12796.5068 | Reconstruction: 37.3775 | kl_div: 18407.5312| SSIM: 0.2914 \n",
      "2023-08-11 20:39:33,841 - INFO - Step: 0071  | NELBO: 17093.9043 | Reconstruction: 37.3972 | kl_div: 24607.3398| SSIM: 0.2877 \n",
      "2023-08-11 20:39:33,884 - INFO - Step: 0072  | NELBO: 7673.6016 | Reconstruction: 37.4736 | kl_div: 11016.6035| SSIM: 0.2900 \n",
      "2023-08-11 20:39:33,933 - INFO - Step: 0073  | NELBO: 13695.6660 | Reconstruction: 37.4863 | kl_div: 19704.5879| SSIM: 0.2953 \n",
      "2023-08-11 20:39:33,976 - INFO - Step: 0074  | NELBO: 2292.4685 | Reconstruction: 37.4822 | kl_div: 3253.2576| SSIM: 0.2922 \n",
      "2023-08-11 20:39:34,020 - INFO - Step: 0075  | NELBO: 8219.9111 | Reconstruction: 37.3762 | kl_div: 11804.9023| SSIM: 0.2894 \n",
      "2023-08-11 20:39:34,068 - INFO - Step: 0076  | NELBO: 1639.5168 | Reconstruction: 37.4260 | kl_div: 2311.3284| SSIM: 0.2904 \n",
      "2023-08-11 20:39:34,116 - INFO - Step: 0077  | NELBO: 27887.4629 | Reconstruction: 37.2926 | kl_div: 40179.3008| SSIM: 0.2909 \n",
      "2023-08-11 20:39:34,117 - INFO - \n",
      "-----------------------------------------------------------------------------------------\n",
      "2023-08-11 20:39:34,117 - INFO - NELBO: 2914134.500000 | SSIM: 0.290426\n"
     ]
    }
   ],
   "source": [
    "from synthesize import main\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "net = torch.nn.Sequential()\n",
    "trainable_h = torch.nn.Parameter(  # for unconditional generation\n",
    "    data=torch.empty(size=(1, 1000)), requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "y = torch.tile(trainable_h, (32, 1, 1, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 1, 1, 1000])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "y = net(trainable_h)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1000])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "y1, y2 = y.chunk(2, dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
